# Machine Comprehension


## Span-based

### Reading

* [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://aclweb.org/anthology/D16-1264), Rajpurkar et al., EMNLP, 2016.
* [TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](http://aclweb.org/anthology/P17-1147), Joshi et al., ACL, 2017.
* [NewsQA: A Machine Comprehension Dataset](http://www.aclweb.org/anthology/W17-2623), Trischler et al., RepL4NLP, 2017.
* [Reading Wikipedia to Answer Open-Domain Questions](http://aclweb.org/anthology/P17-1171), Chen et al., ACL, 2017.
* R-NET: [Gated Self-Matching Networks for Reading Comprehension and Question Answering](http://www.aclweb.org/anthology/P17-1018), Wang et al., ACL, 2017.

### Presentation

* SLQA: [Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering](http://aclweb.org/anthology/P18-1158), Wang et al., ACL, 2018.
* [QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541), Yu et al., ICLR, 2018.


## Cloze-style

### Reading

* CNN/Daily Mail: [Teaching Machines to Read and Comprehend](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend), Hermann et al., NIPS, 2015.
* Children's Book Test: [The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations](https://arxiv.org/abs/1511.02301), Hill et al., ICLR, 2016.
* [The LAMBADA dataset: Word prediction requiring a broad discourse context](https://www.aclweb.org/anthology/P16-1144), Paperno et al., ACL, 2016.
* [Who did What: A Large-Scale Person-Centered Cloze Dataset](http://www.aclweb.org/anthology/D16-1241), Onishi et al., EMNLP, 2016.
* [A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task](https://www.aclweb.org/anthology/P16-1223), Chen et al., ACL, 2016.

### Presentation

* [Attention-over-Attention Neural Networks for Reading Comprehension](http://aclweb.org/anthology/P17-1055), Cui et al., ACL, 2017.
* [Gated-Attention Readers for Text Comprehension](http://aclweb.org/anthology/P17-1168), Dhingra et al., ACL, 2017.


## Others

### Reading

* [RACE: Large-scale ReAding Comprehension Dataset From Examinations](http://aclweb.org/anthology/D17-1082), Lai et al., EMNLP, 2017.
* [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/abs/1611.09268), Nguyen et al., ICLR, 2017.
* [SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine](https://arxiv.org/abs/1704.05179), Dunn et al., arXiv, 2017.
* [The NarrativeQA Reading Comprehension Challenge](http://aclweb.org/anthology/Q18-1023), Kočiský et al., TACL, 2018.
* [Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog](http://www.aclweb.org/anthology/N18-1185), Ma et al., NAACL, 2018.

### Presentation

* SAN: [Stochastic Answer Networks for Machine Reading Comprehension](http://aclweb.org/anthology/P18-1157), Liu et al., ACL, 2018.
* [Attention-Guided Answer Distillation for Machine Reading Comprehension](http://aclweb.org/anthology/D18-1232), Hu et al., EMNLP, 2018.
